# Default configuration aligned with protobuf service definitions
output_dir: results/raw/${experiment_id}

target_model:
  hf_path: "meta-llama/Llama-3.1-70B"
  generation_config:
    temperatures: [0, 1]

drafter_models:
  - hf_path: "meta-llama/Llama-3.2-1B-Instruct"
    generation_config:
      temperatures: [0, 1]
  - hf_path: "meta-llama/Llama-3.2-3B-Instruct"
    generation_config:
      temperatures: [0, 1]
  - hf_path: "meta-llama/Llama-3.1-8B"
    generation_config:
      temperatures: [0, 1]
  - hf_path: "Qwen/Qwen2.5-0.5B-Instruct"
    generation_config:
      temperatures: [0, 1e-7, 1]

task:
  dataset:
    hf_path: "tau/scrolls"
    name: "qasper"
    split: "test"
    num_examples: 30
  max_new_tokens: 512


# Cluster-specific configurations
cluster_config:
  type: "lsf"  # or "slurm"

  lsf:
    hardware_request:
      # GPU settings from -gpu flag
      gpu:
        type: null  # Unspecified by default
        count: 1
        memory_gb: 80
        is_exclusive: true

      # CPU & Memory settings
      cpu_cores: 8        # From -R "affinity[core(8)]"
      memory_gb: 200      # From both -R "rusage[mem=200GB]" and -M 200GB
      
      # Queue settings
      queue_names: ["long-gpu", "short-gpu", "risk-gpu"]
      num_hosts: 1        # From -R "span[hosts=1]"
      num_processes: 1    # From -n 1

    environment:
      # Environment modules
      modules:
        - "miniconda/24.11_environmentally"
        - "CUDA/12.4.0"

  slurm:
    ...

logging:
  level: DEBUG
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  output_dir: "~/hf_bench_logs/"
  filename_pattern: "${timestamp}_jobid_%J_benchmark.log"

metrics:
  - num_new_toks
  - ttft
  - tpot_hmean_ms
  - tpot_min_ms
  - output_tokens_per_sec_hmean
  - output_tokens_per_sec_max
  - output_tokens_per_sec_std
  - per_example_speedup_over_ar_star_hmean
  - per_example_speedup_over_ar_star_max
  - per_example_speedup_over_ar_star_std
